{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1acc12a-712e-41e6-8e30-41f9de223543",
   "metadata": {},
   "source": [
    "# Multimodal RAG with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c557723-257f-4746-9b84-ec77c50cf405",
   "metadata": {},
   "source": [
    "This cookbook shows how to perform RAG on the table and text extraction output of nv-ingest's pdf extraction tools using LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecfda5-137b-43da-a8d4-23dd47131be9",
   "metadata": {},
   "source": [
    "To start we'll need to make sure we have llama_index installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75fdaa-3085-4b77-9289-15276d5cd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU llama_index llama-index-embeddings-nvidia llama-index-llms-nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45412661-9516-47f9-8bea-6f857e0e173f",
   "metadata": {},
   "source": [
    "Then, we'll use nv-ingest to parse an example pdf that contains text, tables, charts, and images. We'll need to make sure to have the nv-ingest microservice up and running at localhost:7670 along with the supporting NIMs. To do this, follow the nv-ingest [quickstart guide](https://github.com/NVIDIA/nv-ingest?tab=readme-ov-file#quickstart). Once the microservice is ready we can create a job with the nv-ingest python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9337daf9-7342-427a-a95e-2d9ac9b5076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nv_ingest_client.client import NvIngestClient\n",
    "from nv_ingest_client.message_clients.rest.rest_client import RestClient\n",
    "from nv_ingest_client.primitives import JobSpec\n",
    "from nv_ingest_client.primitives.tasks import ExtractTask\n",
    "\n",
    "\n",
    "from nv_ingest_client.util.file_processing.extract import extract_file_content\n",
    "import logging, time\n",
    "\n",
    "logger = logging.getLogger(\"nv_ingest_client\")\n",
    "\n",
    "file_name = \"../data/multimodal_test.pdf\"\n",
    "file_content, file_type = extract_file_content(file_name)\n",
    "\n",
    "job_spec = JobSpec(\n",
    "    document_type=file_type,\n",
    "    payload=file_content,\n",
    "    source_id=file_name,\n",
    "    source_name=file_name,\n",
    "    extended_options={\n",
    "        \"tracing_options\": {\n",
    "            \"trace\": True,\n",
    "            \"ts_send\": time.time_ns()\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9a74c-f7e4-475d-970d-cf820cd8ea19",
   "metadata": {},
   "source": [
    "And then we can and submit a task to extract the text and tables from the example pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f07d820-0ae6-4681-88da-3f8857ea71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_task = ExtractTask(\n",
    "    document_type=file_type,\n",
    "    extract_text=True,\n",
    "    extract_images=False,\n",
    "    extract_tables=True,\n",
    ")\n",
    "\n",
    "\n",
    "job_spec.add_task(extract_task)\n",
    "\n",
    "client = NvIngestClient(\n",
    "  message_client_hostname=\"localhost\",\n",
    "  message_client_port=7670\n",
    ")\n",
    "\n",
    "job_id = client.add_job(job_spec)\n",
    "\n",
    "client.submit_job(job_id, \"morpheus_task_queue\")\n",
    "\n",
    "result = client.fetch_job_result(job_id, timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a0a9c-ede6-4cef-b182-9f0b78223647",
   "metadata": {},
   "source": [
    "Now, we have the extraction results in the nv-ingest metadata format. We'll separate the content out of this and load it into LlamaIndex documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b0d5eb-f0db-4edb-a3fe-c3bfccc59227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "texts = []\n",
    "tables = []\n",
    "for element in result[0]:\n",
    "    if element['document_type'] == 'text':\n",
    "        texts.append(Document(text=element['metadata']['content']))\n",
    "    elif element['document_type'] == 'structured':\n",
    "        tables.append(Document(text=element['metadata']['table_metadata']['table_content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f220a41-fc55-4b6c-95e1-28b41bfdba0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='f8eb55dc-6c04-4898-a892-a363c9e25a11', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='TestingDocument\\r\\nA sample document with headings and placeholder text\\r\\nIntroduction\\r\\nThis is a placeholder document that can be used for any purpose. It contains some \\r\\nheadings and some placeholder text to fill the space. The text is not important and contains \\r\\nno real value, but it is useful for testing. Below, we will have some simple tables and charts \\r\\nthat we can use to confirm Ingest is working as expected.\\r\\nTable 1\\r\\nThis table describes some animals, and some activities they might be doing in specific \\r\\nlocations.\\r\\nAnimal Activity Place\\r\\nGira@e Driving a car At the beach\\r\\nLion Putting on sunscreen At the park\\r\\nCat Jumping onto a laptop In a home o@ice\\r\\nDog Chasing a squirrel In the front yard\\r\\nChart 1\\r\\nThis chart shows some gadgets, and some very fictitious costs. Section One\\r\\nThis is the first section of the document. It has some more placeholder text to show how \\r\\nthe document looks like. The text is not meant to be meaningful or informative, but rather to \\r\\ndemonstrate the layout and formatting of the document.\\r\\n• This is the first bullet point\\r\\n• This is the second bullet point\\r\\n• This is the third bullet point\\r\\nSection Two\\r\\nThis is the second section of the document. It is more of the same as we’ve seen in the rest \\r\\nof the document. The content is meaningless, but the intent is to create a very simple \\r\\nsmoke test to ensure extraction is working as intended. This will be used in CI as time goes \\r\\non to ensure that changes we make to the library do not negatively impact our accuracy.\\r\\nTable 2\\r\\nThis table shows some popular colors that cars might come in.\\r\\nCar Color1 Color2 Color3\\r\\nCoupe White Silver Flat Gray\\r\\nSedan White Metallic Gray Matte Gray\\r\\nMinivan Gray Beige Black\\r\\nTruck Dark Gray Titanium Gray Charcoal\\r\\nConvertible Light Gray Graphite Slate Gray\\r\\nPicture\\r\\nBelow, is a high-quality picture of some shapes. Chart 2\\r\\nThis chart shows some average frequency ranges for speaker drivers.\\r\\nConclusion\\r\\nThis is the conclusion of the document. It has some more placeholder text, but the most \\r\\nimportant thing is that this is the conclusion. As we end this document, we should have \\r\\nbeen able to extract 2 tables, 2 charts, and some text including 3 bullet points.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c0f34-0af7-4092-9801-f18102973c3b",
   "metadata": {},
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6a9b8-83cf-4061-bf57-d50edc3978d0",
   "metadata": {},
   "source": [
    "Now, the text and table content is ready to be embedded and stored. We'll set our OpenAI api key in order to use OpenAI's embedding model, but any desired embedding model can be used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d693a4-e647-4c20-bea4-56fe52c74541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "\n",
    "# TODO: Add your NVIDIA API key here\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"<YOUR_NVIDIA_API_KEY>\"\n",
    "\n",
    "embed_model = NVIDIAEmbedding(model=\"NV-Embed-QA\")\n",
    "index = VectorStoreIndex.from_documents(texts+tables, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e23e9d-a3ad-4b80-a356-1b233633b82d",
   "metadata": {},
   "source": [
    "Next, we'll use our vectorstore to create a query engine that handles the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3eb210e-1106-4956-80a3-f950d30ac6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.nvidia import NVIDIA\n",
    "\n",
    "llm = NVIDIA(model=\"meta/llama-3.1-405b-instruct\")\n",
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664333b-d30a-4846-801d-e484a18efe45",
   "metadata": {},
   "source": [
    "And finally, we can ask it questions about our example PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f397a34b-4639-4f8a-81a8-5a5a416404d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog is chasing a squirrel in the front yard.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"What is the dog doing and where?\").response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a521b-aca8-4227-b73d-1c3fe8aef95b",
   "metadata": {},
   "source": [
    "Alternatively, we can use the milvus vector database packaged with NV-Ingest. This requires pymilvus and the milvus, etcd, attu, and minio microservices to be up and [running](https://github.com/NVIDIA/nv-ingest/blob/main/docs/deployment.md#launch-nv-ingest-micro-services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "572b2b1a-367b-41bb-a153-ad86227fa815",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU pymilvus llama-index-vector-stores-milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a9658c-6d5b-4cb0-ace2-2ef59cb8fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"http://localhost:19530\",\n",
    "    dim=1024,\n",
    "    overwrite=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(texts+tables, storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae32590b-2ff3-4613-b85f-75942b1b5eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d30cd73a-88e1-45cb-94e6-3d07deffe0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog is chasing a squirrel in the front yard.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"What is the dog doing and where?\").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2979c08-56a0-441f-87bf-2c6d0c8391a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
