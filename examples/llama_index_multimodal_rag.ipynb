{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1acc12a-712e-41e6-8e30-41f9de223543",
   "metadata": {},
   "source": [
    "# Multimodal RAG with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c557723-257f-4746-9b84-ec77c50cf405",
   "metadata": {},
   "source": [
    "This notebook shows how to perform RAG on the table, chart, and text extraction results of nv-ingest's pdf extraction tools using LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecfda5-137b-43da-a8d4-23dd47131be9",
   "metadata": {},
   "source": [
    "To start we'll need to make sure we have LlamaIndex installed as well as pymilvus so that we can connect to the Milvus vector database (VDB) that NV-Ingest uses to store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75fdaa-3085-4b77-9289-15276d5cd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU llama_index llama-index-embeddings-nvidia llama-index-llms-nvidia llama-index-vector-stores-milvus pymilvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45412661-9516-47f9-8bea-6f857e0e173f",
   "metadata": {},
   "source": [
    "Then, we'll use NV-Ingest to parse an example pdf that contains text, tables, charts, and images, embed it with the included embedding microservice and store the results in the Milvus vector database. We'll need to make sure to have the NV-Ingest microservice up and running at localhost:7670 along with the supporting NIMs and microservices. To do this, follow the nv-ingest [quickstart guide](https://github.com/NVIDIA/nv-ingest?tab=readme-ov-file#quickstart). This notebook requires all of the services to be [running](https://github.com/NVIDIA/nv-ingest/blob/main/docs/deployment.md#launch-nv-ingest-micro-services). Once everything is ready, we can create a job with the NV-Ingest python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9337daf9-7342-427a-a95e-2d9ac9b5076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nv_ingest_client.client import NvIngestClient\n",
    "from nv_ingest_client.primitives import JobSpec\n",
    "from nv_ingest_client.primitives.tasks import ExtractTask\n",
    "from nv_ingest_client.primitives.tasks import EmbedTask\n",
    "from nv_ingest_client.primitives.tasks import VdbUploadTask\n",
    "\n",
    "\n",
    "from nv_ingest_client.util.file_processing.extract import extract_file_content\n",
    "import logging, time\n",
    "\n",
    "logger = logging.getLogger(\"nv_ingest_client\")\n",
    "\n",
    "file_name = \"../data/multimodal_test.pdf\"\n",
    "file_content, file_type = extract_file_content(file_name)\n",
    "\n",
    "client = NvIngestClient(\n",
    "  message_client_hostname=\"localhost\",\n",
    "  message_client_port=7670\n",
    ")\n",
    "\n",
    "job_spec = JobSpec(\n",
    "    document_type=file_type,\n",
    "    payload=file_content,\n",
    "    source_id=file_name,\n",
    "    source_name=file_name,\n",
    "    extended_options={\n",
    "        \"tracing_options\": {\n",
    "            \"trace\": True,\n",
    "            \"ts_send\": time.time_ns()\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9a74c-f7e4-475d-970d-cf820cd8ea19",
   "metadata": {},
   "source": [
    "And then, we can add and submit tasks to extract the text, tables, and charts from the example pdf, generate embeddings from the results, and store them in the Milvus VDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f07d820-0ae6-4681-88da-3f8857ea71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_task = ExtractTask(\n",
    "    document_type=file_type,\n",
    "    extract_text=True,\n",
    "    extract_images=False,\n",
    "    extract_tables=True,\n",
    "    extract_charts=True,\n",
    ")\n",
    "\n",
    "embed_task = EmbedTask(\n",
    "    text=True,\n",
    "    tables=True,\n",
    ")\n",
    "\n",
    "vdb_upload_task = VdbUploadTask()\n",
    "\n",
    "job_spec.add_task(extract_task)\n",
    "job_spec.add_task(embed_task)\n",
    "job_spec.add_task(vdb_upload_task)\n",
    "\n",
    "job_id = client.add_job(job_spec)\n",
    "\n",
    "client.submit_job(job_id, \"morpheus_task_queue\")\n",
    "\n",
    "result = client.fetch_job_result(job_id, timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6a9b8-83cf-4061-bf57-d50edc3978d0",
   "metadata": {},
   "source": [
    "Now, the text, table, and chart content is extracted and stored in the Milvus VDB along with the embeddings. Next, we'll connect LlamaIndex to Milvus and create a vector store index so that we can query our extraction results. The vector store index must use the same embedding model as the embedding service in NV-Ingest: `nv-embed-qa-e5-v5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d693a4-e647-4c20-bea4-56fe52c74541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "embed_model = NVIDIAEmbedding(base_url=\"http://localhost:8012/v1\", model=\"nvidia/nv-embedqa-e5-v5\")\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"http://localhost:19530\",\n",
    "    collection_name=\"nv_ingest_collection\",\n",
    "    doc_id_field=\"pk\",\n",
    "    embedding_field=\"vector\",\n",
    "    text_key=\"text\",\n",
    "    dim=1024,\n",
    "    overwrite=False\n",
    ")\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e23e9d-a3ad-4b80-a356-1b233633b82d",
   "metadata": {},
   "source": [
    "Next, we'll use our vector store index to create a query engine that handles the RAG pipeline and we'll use an LLM from the NVIDIA API catalog to generate the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3eb210e-1106-4956-80a3-f950d30ac6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.nvidia import NVIDIA\n",
    "\n",
    "llm = NVIDIA(model=\"meta/llama-3.1-405b-instruct\")\n",
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664333b-d30a-4846-801d-e484a18efe45",
   "metadata": {},
   "source": [
    "And finally, we can ask it questions about our example PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f397a34b-4639-4f8a-81a8-5a5a416404d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog is chasing a squirrel in the front yard.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"What is the dog doing and where?\").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2979c08-56a0-441f-87bf-2c6d0c8391a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
