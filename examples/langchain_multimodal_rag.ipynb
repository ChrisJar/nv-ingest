{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7efe0f92-fdbb-4471-b74c-5bdaafed8102",
   "metadata": {},
   "source": [
    "# Multimodal RAG with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ece9e3-155a-44f4-81e5-2f9492c62a2f",
   "metadata": {},
   "source": [
    "This notebook shows how to perform RAG on the table, chart, and text extraction results of nv-ingest's pdf extraction tools using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81014734-f765-48fc-8fc2-4c19f5f28eae",
   "metadata": {},
   "source": [
    "To start we'll need to make sure we have Langchain installed as well as pymilvus so that we can connect to the Milvus vector database (VDB) that NV-Ingest uses to store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacbe052-4429-4c0a-8b1e-309ac55ad8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU langchain langchain_community langchain-nvidia-ai-endpoints langchain_milvus pymilvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888ba26-04cf-4577-81a3-5bcd537fc2f6",
   "metadata": {},
   "source": [
    "Then, we'll use NV-Ingest to parse an example pdf that contains text, tables, charts, and images, embed it with the included embedding microservice and store the results in the Milvus vector database. We'll need to make sure to have the NV-Ingest microservice up and running at localhost:7670 along with the supporting NIMs and microservices. To do this, follow the nv-ingest [quickstart guide](https://github.com/NVIDIA/nv-ingest?tab=readme-ov-file#quickstart). This notebook requires all of the services to be [running](https://github.com/NVIDIA/nv-ingest/blob/main/docs/deployment.md#launch-nv-ingest-micro-services). Once everything is ready, we can create a job with the NV-Ingest python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32017922-9b9c-48b9-86ab-6319377fcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nv_ingest_client.client import NvIngestClient\n",
    "from nv_ingest_client.message_clients.rest.rest_client import RestClient\n",
    "from nv_ingest_client.primitives import JobSpec\n",
    "from nv_ingest_client.primitives.tasks import ExtractTask\n",
    "from nv_ingest_client.primitives.tasks import EmbedTask\n",
    "from nv_ingest_client.primitives.tasks import VdbUploadTask\n",
    "\n",
    "\n",
    "from nv_ingest_client.util.file_processing.extract import extract_file_content\n",
    "import logging, time\n",
    "\n",
    "logger = logging.getLogger(\"nv_ingest_client\")\n",
    "\n",
    "file_name = \"../data/multimodal_test.pdf\"\n",
    "file_content, file_type = extract_file_content(file_name)\n",
    "\n",
    "client = NvIngestClient(\n",
    "  message_client_hostname=\"localhost\",\n",
    "  message_client_port=7670\n",
    ")\n",
    "\n",
    "job_spec = JobSpec(\n",
    "    document_type=file_type,\n",
    "    payload=file_content,\n",
    "    source_id=file_name,\n",
    "    source_name=file_name,\n",
    "    extended_options={\n",
    "        \"tracing_options\": {\n",
    "            \"trace\": True,\n",
    "            \"ts_send\": time.time_ns()\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa987e6-8bb1-4ed1-97c2-540e136e2d19",
   "metadata": {},
   "source": [
    "And then, we can add and submit tasks to extract the text, tables, and charts from the example pdf, generate embeddings from the results, and store them in the Milvus VDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bcd6b2a-c832-4685-bd6e-53b60a107e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_task = ExtractTask(\n",
    "    document_type=file_type,\n",
    "    extract_text=True,\n",
    "    extract_images=False,\n",
    "    extract_tables=True,\n",
    "    extract_charts=True,\n",
    ")\n",
    "\n",
    "embed_task = EmbedTask(\n",
    "    text=True,\n",
    "    tables=True,\n",
    ")\n",
    "\n",
    "vdb_upload_task = VdbUploadTask()\n",
    "\n",
    "job_spec.add_task(extract_task)\n",
    "job_spec.add_task(embed_task)\n",
    "job_spec.add_task(vdb_upload_task)\n",
    "\n",
    "job_id = client.add_job(job_spec)\n",
    "\n",
    "client.submit_job(job_id, \"morpheus_task_queue\")\n",
    "\n",
    "result = client.fetch_job_result(job_id, timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02131711-31bf-4536-81b7-8c464c7473e3",
   "metadata": {},
   "source": [
    "Now, the text, table, and chart content is extracted and stored in the Milvus VDB along with the embeddings. Next we'll connect LlamaIndex to Milvus and create a vector store so that we can query our extraction results. The vector store must use the same embedding model as the embedding service in NV-Ingest: `nv-embed-qa-e5-v5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53957974-c688-4521-8c61-09f2649d5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "# TODO: Add your NVIDIA API key here\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"<YOUR_NVIDIA_API_KEY>\"\n",
    "\n",
    "embedding = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\")\n",
    "\n",
    "vectorstore = Milvus(\n",
    "    embedding_function=embedding,\n",
    "    collection_name=\"nv_ingest_collection\",\n",
    "    primary_field = \"pk\",\n",
    "    vector_field = \"vector\",\n",
    "    text_field=\"text\",\n",
    "    connection_args={\"uri\": \"http://localhost:19530\"},\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87111b5-e5a8-45a0-9663-2ae6d9ea2ab6",
   "metadata": {},
   "source": [
    "Finally, we'll create an RAG chain that we can use to query our pdf in natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b547a19a-9ada-4a40-a246-6d7bc4d24482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog is chasing a squirrel in the front yard.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Keep the answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"What is the dog doing and where?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1200811-b4ac-468e-bb32-77c102aa9e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
